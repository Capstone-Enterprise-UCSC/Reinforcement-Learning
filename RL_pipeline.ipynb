{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/7c/55/b3432f43d6d7fee999bb23a547820d74c48ec540f5f7842e41aa5d8d5f3a/datasets-2.14.6-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/be/42/f08b6ccec46df249969c06c66853d2e146c50e0ffd133dfebaf9b53a03c4/pyarrow-14.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pyarrow-14.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/ad/80/8fc9a4d76b259c901f2c85ed10f330a8fb51993a577bddfd53a852595e12/xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c2/a6/c5cb599d917904878f220a4dbdfdcc4ef291dd3956c35b3b0dc6fc42fb6d/multiprocess-0.70.15-py38-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from datasets) (2023.9.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/91/1b/9b5da3748ddd924288d9a946a0f48a34b179330d342c6cdeb89683bcf971/aiohttp-3.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/0b/36/c276486f89bee9098332710af2207344f360c6c6f104a4931a7566039b1d/frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tzdata, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.6 dill-0.3.7 frozenlist-1.4.0 multidict-6.0.4 multiprocess-0.70.15 pandas-2.0.3 pyarrow-14.0.0 tzdata-2023.3 xxhash-3.4.1 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unbabel-comet\n",
      "  Obtaining dependency information for unbabel-comet from https://files.pythonhosted.org/packages/d7/56/ed6119409bae7ac566b2b9475cddfc82cb2a6e8166a3b635b1b0b4fd8089/unbabel_comet-2.2.0-py3-none-any.whl.metadata\n",
      "  Downloading unbabel_comet-2.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting entmax<2.0,>=1.1 (from unbabel-comet)\n",
      "  Downloading entmax-1.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: huggingface-hub<0.17.0,>=0.16.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from unbabel-comet) (0.16.4)\n",
      "Collecting jsonargparse==3.13.1 (from unbabel-comet)\n",
      "  Downloading jsonargparse-3.13.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from unbabel-comet) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from unbabel-comet) (2.0.3)\n",
      "Collecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n",
      "  Obtaining dependency information for protobuf<5.0.0,>=4.24.4 from https://files.pythonhosted.org/packages/ae/32/45b1cf0c5d4a3ba881f5164c26af877c0dabfe6de0019d426aa0e5cf6806/protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting pytorch-lightning<3.0.0,>=2.0.0 (from unbabel-comet)\n",
      "  Obtaining dependency information for pytorch-lightning<3.0.0,>=2.0.0 from https://files.pythonhosted.org/packages/98/09/9b2eab7833494e7c82f70c9b2f8e907d38231f4535704e3045a8a4960c8e/pytorch_lightning-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_lightning-2.1.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting sacrebleu<3.0.0,>=2.0.0 (from unbabel-comet)\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy<2.0.0,>=1.5.4 (from unbabel-comet)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece<0.2.0,>=0.1.96 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from unbabel-comet) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from unbabel-comet) (2.1.0)\n",
      "Collecting torchmetrics<0.11.0,>=0.10.2 (from unbabel-comet)\n",
      "  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0,>=4.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from unbabel-comet) (4.34.0)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0)\n",
      "Requirement already satisfied: filelock in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (3.12.4)\n",
      "Requirement already satisfied: fsspec in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (2023.9.2)\n",
      "Requirement already satisfied: requests in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from pandas>=1.4.1->unbabel-comet) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from pandas>=1.4.1->unbabel-comet) (2023.3)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
      "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/46/ee/8641eeb6a062f383b7d6875604e1f3f83bd2c93a0b4dbcabd3150b32de6e/lightning_utilities-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting portalocker (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n",
      "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2023.10.3)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting colorama (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (4.9.3)\n",
      "Requirement already satisfied: sympy in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (1.12)\n",
      "Requirement already satisfied: networkx in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.6.0->unbabel-comet) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->unbabel-comet) (12.2.140)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.3.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from fsspec->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (3.8.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->unbabel-comet) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from sympy->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub<0.17.0,>=0.16.0->unbabel-comet) (1.3.1)\n",
      "Downloading unbabel_comet-2.2.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.1.0-py3-none-any.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.6/774.6 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: tabulate, scipy, protobuf, portalocker, lightning-utilities, jsonargparse, colorama, sacrebleu, torchmetrics, entmax, pytorch-lightning, unbabel-comet\n",
      "Successfully installed colorama-0.4.6 entmax-1.1 jsonargparse-3.13.1 lightning-utilities-0.9.0 portalocker-2.8.2 protobuf-4.25.0 pytorch-lightning-2.1.0 sacrebleu-2.3.1 scipy-1.10.1 tabulate-0.9.0 torchmetrics-0.10.3 unbabel-comet-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from accelerate) (0.16.4)\n",
      "Requirement already satisfied: filelock in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.2.140)\n",
      "Requirement already satisfied: requests in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from transformers[torch]) (0.24.1)\n",
      "Requirement already satisfied: psutil in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.12.0,>=1.10->transformers[torch]) (12.2.140)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers[torch] -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric, load_from_disk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, TrainingArguments\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'm2m100_418M'\n",
    "experiment = 'en-ha-finetune'\n",
    "#dataset_name = '/home/spandey7/Language-Adapters/Data/en-ha' #change this to the name of the folder containing the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"facebook/{model_name}\")\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"facebook/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'ha'\n",
    "tgt_lang = 'en'\n",
    "tokenizer.src_lang = \"ha\"\n",
    "tokenizer.tgt_lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/spandey7/active-learning/RL_pipeline.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.be.ucsc.edu/home/spandey7/active-learning/RL_pipeline.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the datasets from Hugging Face Hub\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.be.ucsc.edu/home/spandey7/active-learning/RL_pipeline.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mpranjali97/ha-en_RL-grow1_train\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.be.ucsc.edu/home/spandey7/active-learning/RL_pipeline.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mpranjali97/ha-en_RL-grow1_valid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2130\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   2131\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2135\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2139\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2141\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   2142\u001b[0m )\n\u001b[1;32m   2144\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1816\u001b[0m     path,\n\u001b[1;32m   1817\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1818\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1819\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1820\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1821\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1822\u001b[0m )\n\u001b[1;32m   1823\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/load.py:1489\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1482\u001b[0m             path,\n\u001b[1;32m   1483\u001b[0m             revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1486\u001b[0m             dynamic_modules_path\u001b[39m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1487\u001b[0m         )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1488\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1489\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1490\u001b[0m             path,\n\u001b[1;32m   1491\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1492\u001b[0m             data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1493\u001b[0m             data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1494\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1495\u001b[0m             download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1496\u001b[0m         )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1497\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m   1498\u001b[0m     \u001b[39mException\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m ) \u001b[39mas\u001b[39;00m e1:  \u001b[39m# noqa all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/load.py:1013\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.__init__\u001b[0;34m(self, name, revision, data_dir, data_files, download_config, download_mode)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_config \u001b[39m=\u001b[39m download_config \u001b[39mor\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1012\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_mode \u001b[39m=\u001b[39m download_mode\n\u001b[0;32m-> 1013\u001b[0m increase_load_count(name, resource_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/load.py:232\u001b[0m, in \u001b[0;36mincrease_load_count\u001b[0;34m(name, resource_type)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mHF_DATASETS_OFFLINE \u001b[39mand\u001b[39;00m config\u001b[39m.\u001b[39mHF_UPDATE_DOWNLOAD_COUNTS:\n\u001b[1;32m    231\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m         head_hf_s3(name, filename\u001b[39m=\u001b[39;49mname \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.py\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataset\u001b[39m=\u001b[39;49m(resource_type \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    233\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/utils/file_utils.py:96\u001b[0m, in \u001b[0;36mhead_hf_s3\u001b[0;34m(identifier, filename, use_cdn, dataset, max_retries)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhead_hf_s3\u001b[39m(\n\u001b[1;32m     94\u001b[0m     identifier: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, use_cdn\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dataset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_retries\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     95\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[requests\u001b[39m.\u001b[39mResponse, \u001b[39mException\u001b[39;00m]:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m http_head(\n\u001b[1;32m     97\u001b[0m         hf_bucket_url(identifier\u001b[39m=\u001b[39;49midentifier, filename\u001b[39m=\u001b[39;49mfilename, use_cdn\u001b[39m=\u001b[39;49muse_cdn, dataset\u001b[39m=\u001b[39;49mdataset),\n\u001b[1;32m     98\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m     99\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/utils/file_utils.py:429\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    427\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    428\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 429\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    430\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    431\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    432\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    433\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    434\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    435\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[1;32m    436\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    437\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    438\u001b[0m )\n\u001b[1;32m    439\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/datasets/utils/file_utils.py:328\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    326\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    329\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1056\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1057\u001b[0m         (\n\u001b[1;32m   1058\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1064\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    365\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the datasets from Hugging Face Hub\n",
    "train_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_train\")\n",
    "valid_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the datasets to only include samples with a score > 0.6\n",
    "train_dataset = train_dataset.filter(lambda example: example['score'] > 0.6)\n",
    "valid_dataset = valid_dataset.filter(lambda example: example['score'] > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the original sentences along with the tokenized inputs and labels\n",
    "    return {'ha': inputs, 'en': targets, 'input_ids': model_inputs['input_ids'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Create the DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset.remove_columns(['ref', 'score']),\n",
    "    'validation': tokenized_valid_dataset.remove_columns(['ref', 'score'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['src', 'ref', 'mt', 'score'],\n",
       "        num_rows: 29454\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset_name = '/home/spandey7/Language-Adapters/Data/en-ha' #change this to the name of the folder containing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset = DatasetDict({'train':Dataset.from_pandas(pd.read_csv(f'{old_dataset_name}/cleaned_train.csv')),'validation':Dataset.from_pandas(pd.read_csv(f'{old_dataset_name}/cleaned_dev.csv'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ha'],\n",
       "        num_rows: 9818\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'ha'],\n",
       "        num_rows: 1113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [example for example in examples[src_lang]]\n",
    "    targets = [example for example in examples[tgt_lang]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99849204e1884d2f8180c88c7e2a0838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e7c14933854dcc9f921af37245cd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9818\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    #eval_accumulation_steps=16,\n",
    "    #fp16=True,\n",
    "    #do_train=True,\n",
    "    #do_eval=True,\n",
    "    #logging_steps=5,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"comet\",  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e64195eab1471ab7352d2ea5367613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.1.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "# Load the COMET model for evaluation\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute metrics function to calculate COMET score\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process texts\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Prepare the data for COMET\n",
    "    comet_data = [\n",
    "        {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
    "        for src, mt, ref in zip(inputs, decoded_preds, decoded_labels)\n",
    "    ]\n",
    "    \n",
    "    # Compute COMET score\n",
    "    comet_scores = comet_model.predict(comet_data, batch_size=16)\n",
    "    comet_score = np.mean([score['score'] for score in comet_scores])\n",
    "    \n",
    "    return {\"comet\": comet_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer with the new compute_metrics function\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1276, in forward\n    outputs = self.model(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1181, in forward\n    decoder_outputs = self.decoder(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1045, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 467, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 246, in forward\n    value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 200, in _shape\n    return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 39.69 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 23.16 GiB is allocated by PyTorch, and 104.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/spandey7/active-learning/RL_pipeline.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.be.ucsc.edu/home/spandey7/active-learning/RL_pipeline.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39madd_callback(EarlyStoppingCallback(early_stopping_patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.be.ucsc.edu/home/spandey7/active-learning/RL_pipeline.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.be.ucsc.edu/home/spandey7/active-learning/RL_pipeline.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2747\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2749\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodule_kwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    111\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1276, in forward\n    outputs = self.model(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1181, in forward\n    decoder_outputs = self.decoder(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1045, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 467, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 246, in forward\n    value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n  File \"/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 200, in _shape\n    return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 39.69 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 23.16 GiB is allocated by PyTorch, and 104.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# Add early stopping callback if desired\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-tuning the model\n",
    "batch_size = 32\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{experiment}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    logging_dir=f\"{experiment}/logs\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=1000,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./base_finetune/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexperiment\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# evaluation_strategy=\"steps\",\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# lr_scheduler_type='constant',\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# gradient_accumulation_steps=4,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# gradient_checkpointing=True,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# predict_with_generate=True,\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_steps=5,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbleu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m AdapterTrainer(\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     29\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m<string>:115\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, include_tokens_per_second)\u001b[0m\n",
      "File \u001b[0;32m~/Downloads/pythonProject/Capstone/venv/lib/python3.9/site-packages/transformers/training_args.py:1442\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1441\u001b[0m ):\n\u001b[0;32m-> 1442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1445\u001b[0m     )\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1457\u001b[0m ):\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1459\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1461\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX)."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./base_finetune/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    warmup_steps=1000,\n",
    "    # lr_scheduler_type='constant',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grow Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grow function\n",
    "def grow(model, input_texts, num_samples=5):\n",
    "    generated_texts = []\n",
    "    for text in input_texts:\n",
    "        # Generate multiple translations for each input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=100)\n",
    "        # do we need many samples or just 1?\n",
    "        outputs = model.generate(**inputs, num_return_sequences=num_samples)\n",
    "        generated_texts.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sentence_bleu to start with. Replace with BleuRT and Comet-QE\n",
    "# def reward_function(predictions, references):\n",
    "#     return sentence_bleu(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def improve(model, generated_texts, original_texts, tokenizer, batch_size=8, num_epochs=1, learning_rate=5e-5):\n",
    "#     # Rank and filter the generated texts using the reward function\n",
    "#     scores = [reward_function(text, original_texts) for text in generated_texts]\n",
    "#     print(len(scores))\n",
    "    \n",
    "#     # Sort the generated texts based on their scores\n",
    "#     sorted_texts = [x for _, x in sorted(zip(scores, generated_texts), key=lambda pair: pair[0], reverse=True)]\n",
    "#     print(sorted_texts)\n",
    "    \n",
    "#     # Use the top-ranked texts for fine-tuning\n",
    "#     # For simplicity, let's use the top 50% of the sorted_texts\n",
    "#     training_data = sorted_texts[:len(sorted_texts) // 2]\n",
    "#     print(training_data)\n",
    "#     return None\n",
    "    \n",
    "    # # Convert texts to DataLoader for training\n",
    "    # inputs = tokenizer(training_data, return_tensors=\"pt\", padding=True, truncation=True, max_length=100)\n",
    "    # dataset = torch.utils.data.TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    # dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # # Define optimizer and scheduler\n",
    "    # optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * num_epochs)\n",
    "    \n",
    "    # # Fine-tuning loop\n",
    "    # model.train()\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     for batch in dataloader:\n",
    "    #         input_ids, attention_mask = batch\n",
    "    #         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    #         loss = outputs.loss\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         scheduler.step()\n",
    "    #         optimizer.zero_grad()\n",
    "    \n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforced Self-Training\n",
    "def reinforced_self_loop(model, unsupervised_data, supervised_data, optimizer, num_iterations):\n",
    "    model.train()\n",
    "    for iteration in range(num_iterations):\n",
    "        # 1. Translate the unsupervised data using the current model\n",
    "        inputs = tokenizer(unsupervised_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model.generate(**inputs)\n",
    "        pseudo_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        # 2. Compute the reward for the pseudo-translations\n",
    "        rewards = [reward_function(pred, [ref]) for pred, ref in zip(pseudo_translations, unsupervised_data)]\n",
    "        # 3. Update the model using the pseudo-translations and their rewards\n",
    "        # This part is tricky since the M2M100 model isn't directly designed for RL.\n",
    "        # You'd typically need to define a custom loss function that incorporates the rewards.\n",
    "        # For simplicity, this step is omitted in this outline.\n",
    "        # 4. Fine-tune the model on the supervised data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Lodaing Function\n",
    "# Here, you'd typically load your data. For the sake of this example, let's use dummy data:\n",
    "unsupervised_data = [\"This is an unsupervised sentence.\"] * 10\n",
    "supervised_data = [(\"This is a source sentence.\", \"This is a target sentence.\")] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "dev = pd.read_csv('en-ha/cleaned_dev.csv')\n",
    "train = pd.read_csv('en-ha/cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create finetuned model\n",
    "\n",
    "# Load pre-trained M2M100 model and tokenizer\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call grow step\n",
    "# call reward function and score samples\n",
    "# call improve step\n",
    "    # for th in threshold\n",
    "        #  fine-tune model and check performance for improvement\n",
    "# return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train the model using Reinforced Self-Training\n",
    "reinforced_self_training(model, unsupervised_data, supervised_data, optimizer, num_iterations=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
