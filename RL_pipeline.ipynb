{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers[torch] -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric, load_from_disk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, TrainingArguments\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'm2m100_418M'\n",
    "experiment = 'en-ha-finetune'\n",
    "#dataset_name = '/home/spandey7/Language-Adapters/Data/en-ha' #change this to the name of the folder containing the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"facebook/{model_name}\")\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"facebook/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'ha'\n",
    "tgt_lang = 'en'\n",
    "tokenizer.src_lang = \"ha\"\n",
    "tokenizer.tgt_lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from Hugging Face Hub\n",
    "train_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_train\", split='train')\n",
    "valid_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_valid\", split='train')  # Assuming the split is also 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the datasets to only include samples with a score > 0.6\n",
    "train_dataset = train_dataset.filter(lambda example: example['score'] > 0.6)\n",
    "valid_dataset = valid_dataset.filter(lambda example: example['score'] > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the tokenized inputs and labels\n",
    "    return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src', 'ref', 'mt', 'score']  # Specify the correct columns to remove\n",
    ")\n",
    "tokenized_valid_dataset = valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src', 'ref', 'mt', 'score']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "# Create the DatasetDict\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,  # Directly assign the processed dataset\n",
    "    'validation': tokenized_valid_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Old Data Processing. Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset_name = '/home/spandey7/Language-Adapters/Data/en-ha' #change this to the name of the folder containing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset = DatasetDict({'train':Dataset.from_pandas(pd.read_csv(f'{old_dataset_name}/cleaned_train.csv')),'validation':Dataset.from_pandas(pd.read_csv(f'{old_dataset_name}/cleaned_dev.csv'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_preprocess_function(examples):\n",
    "    inputs = [example for example in examples[src_lang]]\n",
    "    targets = [example for example in examples[tgt_lang]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenized_dataset = old_dataset.map(old_preprocess_function, batched=True, remove_columns=old_dataset.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    eval_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"bleu\",  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process texts\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_scores = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        # Tokenize the sentences into words\n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = [label.split()]  # BLEU expects a list of possible references\n",
    "        # Calculate BLEU score for the sentence pair\n",
    "        # Use a smoothing function to avoid issues with 0 scores for small sentences\n",
    "        score = sentence_bleu(label_tokens, pred_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "        bleu_scores.append(score)\n",
    "    \n",
    "    # Calculate the average BLEU score\n",
    "    average_bleu_score = np.mean(bleu_scores)\n",
    "    \n",
    "    return {\"bleu\": average_bleu_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer with the new compute_metrics function\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add early stopping callback if desired\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "torch.cuda.empty_cache()\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc1bbd594324c91aab380b7ebe7c874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/29454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b77a000e06542a3983b6d06fc811703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the datasets to only include samples with a score > 0.6\n",
    "train_dataset = train_dataset.filter(lambda example: example['score'] > 0.7)\n",
    "valid_dataset = valid_dataset.filter(lambda example: example['score'] > 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2eafd83ea074b5bb4093afdfa363deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e868cb1c185b42579d504cc16149177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/86 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the tokenized inputs and labels\n",
    "    return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src', 'ref', 'mt', 'score']  # Specify the correct columns to remove\n",
    ")\n",
    "tokenized_valid_dataset = valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src', 'ref', 'mt', 'score']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "# Create the DatasetDict\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,  # Directly assign the processed dataset\n",
    "    'validation': tokenized_valid_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 802\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 86\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    eval_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"bleu\",  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process texts\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_scores = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        # Tokenize the sentences into words\n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = [label.split()]  # BLEU expects a list of possible references\n",
    "        # Calculate BLEU score for the sentence pair\n",
    "        # Use a smoothing function to avoid issues with 0 scores for small sentences\n",
    "        score = sentence_bleu(label_tokens, pred_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "        bleu_scores.append(score)\n",
    "    \n",
    "    # Calculate the average BLEU score\n",
    "    average_bleu_score = np.mean(bleu_scores)\n",
    "    \n",
    "    return {\"bleu\": average_bleu_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer with the new compute_metrics function\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/680 12:03 < 14:50, 0.42 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.403500</td>\n",
       "      <td>0.310282</td>\n",
       "      <td>0.030358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>0.255439</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>0.251255</td>\n",
       "      <td>0.019529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.043385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.267700</td>\n",
       "      <td>0.036625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.280107</td>\n",
       "      <td>0.075176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.285017</td>\n",
       "      <td>0.057039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.295427</td>\n",
       "      <td>0.075089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.292727</td>\n",
       "      <td>0.069664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spandey7/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=306, training_loss=0.36162190700522046, metrics={'train_runtime': 731.296, 'train_samples_per_second': 21.934, 'train_steps_per_second': 0.93, 'total_flos': 3910540254511104.0, 'train_loss': 0.36162190700522046, 'epoch': 9.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add early stopping callback if desired\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "torch.cuda.empty_cache()\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-tuning the model\n",
    "batch_size = 32\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{experiment}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    logging_dir=f\"{experiment}/logs\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=1000,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./base_finetune/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    warmup_steps=1000,\n",
    "    # lr_scheduler_type='constant',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grow Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grow function\n",
    "def grow(model, input_texts, num_samples=5):\n",
    "    generated_texts = []\n",
    "    for text in input_texts:\n",
    "        # Generate multiple translations for each input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=100)\n",
    "        # do we need many samples or just 1?\n",
    "        outputs = model.generate(**inputs, num_return_sequences=num_samples)\n",
    "        generated_texts.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sentence_bleu to start with. Replace with BleuRT and Comet-QE\n",
    "# def reward_function(predictions, references):\n",
    "#     return sentence_bleu(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def improve(model, generated_texts, original_texts, tokenizer, batch_size=8, num_epochs=1, learning_rate=5e-5):\n",
    "#     # Rank and filter the generated texts using the reward function\n",
    "#     scores = [reward_function(text, original_texts) for text in generated_texts]\n",
    "#     print(len(scores))\n",
    "    \n",
    "#     # Sort the generated texts based on their scores\n",
    "#     sorted_texts = [x for _, x in sorted(zip(scores, generated_texts), key=lambda pair: pair[0], reverse=True)]\n",
    "#     print(sorted_texts)\n",
    "    \n",
    "#     # Use the top-ranked texts for fine-tuning\n",
    "#     # For simplicity, let's use the top 50% of the sorted_texts\n",
    "#     training_data = sorted_texts[:len(sorted_texts) // 2]\n",
    "#     print(training_data)\n",
    "#     return None\n",
    "    \n",
    "    # # Convert texts to DataLoader for training\n",
    "    # inputs = tokenizer(training_data, return_tensors=\"pt\", padding=True, truncation=True, max_length=100)\n",
    "    # dataset = torch.utils.data.TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    # dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # # Define optimizer and scheduler\n",
    "    # optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * num_epochs)\n",
    "    \n",
    "    # # Fine-tuning loop\n",
    "    # model.train()\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     for batch in dataloader:\n",
    "    #         input_ids, attention_mask = batch\n",
    "    #         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    #         loss = outputs.loss\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         scheduler.step()\n",
    "    #         optimizer.zero_grad()\n",
    "    \n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforced Self-Training\n",
    "def reinforced_self_loop(model, unsupervised_data, supervised_data, optimizer, num_iterations):\n",
    "    model.train()\n",
    "    for iteration in range(num_iterations):\n",
    "        # 1. Translate the unsupervised data using the current model\n",
    "        inputs = tokenizer(unsupervised_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model.generate(**inputs)\n",
    "        pseudo_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        # 2. Compute the reward for the pseudo-translations\n",
    "        rewards = [reward_function(pred, [ref]) for pred, ref in zip(pseudo_translations, unsupervised_data)]\n",
    "        # 3. Update the model using the pseudo-translations and their rewards\n",
    "        # This part is tricky since the M2M100 model isn't directly designed for RL.\n",
    "        # You'd typically need to define a custom loss function that incorporates the rewards.\n",
    "        # For simplicity, this step is omitted in this outline.\n",
    "        # 4. Fine-tune the model on the supervised data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Lodaing Function\n",
    "# Here, you'd typically load your data. For the sake of this example, let's use dummy data:\n",
    "unsupervised_data = [\"This is an unsupervised sentence.\"] * 10\n",
    "supervised_data = [(\"This is a source sentence.\", \"This is a target sentence.\")] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "dev = pd.read_csv('en-ha/cleaned_dev.csv')\n",
    "train = pd.read_csv('en-ha/cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create finetuned model\n",
    "\n",
    "# Load pre-trained M2M100 model and tokenizer\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call grow step\n",
    "# call reward function and score samples\n",
    "# call improve step\n",
    "    # for th in threshold\n",
    "        #  fine-tune model and check performance for improvement\n",
    "# return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train the model using Reinforced Self-Training\n",
    "reinforced_self_training(model, unsupervised_data, supervised_data, optimizer, num_iterations=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
