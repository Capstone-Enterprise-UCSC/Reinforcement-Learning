{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric, load_from_disk, concatenate_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, TrainingArguments, Trainer\n",
    "#from transformers.adapters import AdapterTrainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'm2m100_418M'\n",
    "experiment = 'en-ha_finetune_base_model-1'\n",
    "dataset_name = 'data/en-ha'\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"facebook/{model_name}\")\n",
    "# model = torch.nn.DataParallel(model, device_ids=[2, 3, 4])\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"facebook/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 29454/29454 [00:00<00:00, 188614.68 examples/s]\n",
      "Filter: 100%|██████████| 3339/3339 [00:00<00:00, 155915.31 examples/s]\n",
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from Hugging Face Hub\n",
    "original_train_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_train\", split='train')\n",
    "original_valid_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_valid\", split='train')\n",
    "\n",
    "new_filtered_train_dataset = original_train_dataset.filter(lambda example: example['score'] > 0.8)\n",
    "new_filtered_valid_dataset = original_valid_dataset.filter(lambda example: example['score'] > 0.8)\n",
    "new_filtered_train_dataset = new_filtered_train_dataset.remove_columns(['score', 'ref'])\n",
    "new_filtered_valid_dataset = new_filtered_valid_dataset.remove_columns(['score', 'ref'])\n",
    "\n",
    "train_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_train.csv', split='train')\n",
    "valid_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_dev.csv', split='train')\n",
    "\n",
    "train_dataset = train_dataset.rename_column('ha', 'src')\n",
    "valid_dataset = valid_dataset.rename_column('ha', 'src')\n",
    "train_dataset = train_dataset.rename_column('en', 'mt')\n",
    "valid_dataset = valid_dataset.rename_column('en', 'mt')\n",
    "\n",
    "new_train_dataset = concatenate_datasets([train_dataset, new_filtered_train_dataset])\n",
    "new_valid_dataset = concatenate_datasets([valid_dataset, new_filtered_valid_dataset])  \n",
    "new_train_dataset = new_train_dataset.shuffle(seed=42)\n",
    "new_valid_dataset = new_valid_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mt', 'src'],\n",
       "    num_rows: 9818\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mt', 'src'],\n",
       "    num_rows: 10088\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10088 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 10088/10088 [00:03<00:00, 3306.34 examples/s]\n",
      "Map: 100%|██████████| 1143/1143 [00:00<00:00, 3157.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the tokenized inputs and labels\n",
    "    return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "new_tokenized_train_dataset = new_train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "new_tokenized_valid_dataset = new_valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "# Create the DatasetDict\n",
    "new_tokenized_dataset = DatasetDict({\n",
    "    'train': new_tokenized_train_dataset,  # Directly assign the processed dataset\n",
    "    'validation': new_tokenized_valid_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10088\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1143\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "    # removeme\n",
    "    #import warnings\n",
    "    #warnings.warn(f\"unprocessed preds: {preds[0]}\\n)\")\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) \n",
    "\n",
    "    # removeme\n",
    "    #warnings.warn(f\"unprocessed decoded labels: {tokenizer.batch_decode(labels)[0]}\\n)\")\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # remove me\n",
    "    #inputs = eval_preds.input_ids\n",
    "    #decoded_inputs = tokenizer.batch_decode(inputs)\n",
    "    \n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    bleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    metrics = {\"bleu\": bleu_result[\"score\"]}\n",
    "\n",
    "    flattened_decoded_labels = [' '.join([str(x) for x in l]) for l in decoded_labels]\n",
    "    wer_score = wer.compute(predictions=decoded_preds, references=flattened_decoded_labels)\n",
    "    metrics[\"wer\"] = wer_score\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    metrics[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./lang_adapters/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=15,\n",
    "    warmup_steps=0,\n",
    "    # lr_scheduler_type='cosine_with_restarts',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=new_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=new_tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4740' max='4740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4740/4740 1:01:09, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.290382</td>\n",
       "      <td>24.218600</td>\n",
       "      <td>0.993900</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.263447</td>\n",
       "      <td>27.167900</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.254786</td>\n",
       "      <td>29.091100</td>\n",
       "      <td>0.995200</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.250698</td>\n",
       "      <td>30.134900</td>\n",
       "      <td>0.994400</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.250764</td>\n",
       "      <td>30.991100</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.252950</td>\n",
       "      <td>31.697700</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.256201</td>\n",
       "      <td>31.736300</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.259146</td>\n",
       "      <td>32.244800</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.265032</td>\n",
       "      <td>32.594500</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.269547</td>\n",
       "      <td>32.604000</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.272578</td>\n",
       "      <td>33.177200</td>\n",
       "      <td>0.995200</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.275400</td>\n",
       "      <td>32.932500</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.278461</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.280329</td>\n",
       "      <td>33.167700</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.281060</td>\n",
       "      <td>33.180000</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>34.748000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First of all, we have what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First of all, we have what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First of all, we have what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First of all, we had that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First of all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: First, all, we have that Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: First of all, we know what Gideon and Samuel did not have.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4740, training_loss=0.20453592789072528, metrics={'train_runtime': 3669.8756, 'train_samples_per_second': 41.233, 'train_steps_per_second': 1.292, 'total_flos': 8.198156709789696e+16, 'train_loss': 0.20453592789072528, 'epoch': 15.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = f'./0.8_base_model/{experiment}'\n",
    "\n",
    "if not os.path.exists(base_model_path):\n",
    "    os.makedirs(base_model_path)\n",
    "\n",
    "trainer.save_model(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1012/1012 [00:00<00:00, 3154.70 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 01:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19839/104191538.py:40: UserWarning: preds: WeWe have have 4month burlong womenuten- is no-sabetical and have to have infabetic,\" I heard.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_19839/104191538.py:41: UserWarning: labels: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/test.csv', split='train')\n",
    "\n",
    "#rename 'sentence_eng_Latn' to 'en' and 'sentence_hau_Latn' to 'ha'\n",
    "test_dataset = test_dataset.rename_column('sentence_eng_Latn','mt')\n",
    "test_dataset = test_dataset.rename_column('sentence_hau_Latn','src')\n",
    "\n",
    "#tokenize the dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "#convert the dataset into dataset dict\n",
    "tokenized_test_dataset = DatasetDict({\n",
    "    'test': tokenized_test_dataset  # Directly assign the processed dataset\n",
    "})\n",
    "\n",
    "new_eval_results = trainer.evaluate(tokenized_test_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41771399974823,\n",
       " 'eval_bleu': 15.4553,\n",
       " 'eval_wer': 0.9947,\n",
       " 'eval_gen_len': 34.2757,\n",
       " 'eval_runtime': 104.8212,\n",
       " 'eval_samples_per_second': 9.655,\n",
       " 'eval_steps_per_second': 1.212,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
