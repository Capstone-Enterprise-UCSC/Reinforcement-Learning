{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1572d-e876-433f-a076-07e0e138f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"adapter-transformers@git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3551c529-fcb2-458d-8e5c-4544876eba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric, load_from_disk, concatenate_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, TrainingArguments, Trainer\n",
    "#from transformers.adapters import AdapterTrainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef787507-63c0-45ca-ba66-d56f4141dc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1631e252-26e7-4ec8-ac5b-efdd94a678fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7895b82a-ddd2-481b-8932-0907b73462a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ea527",
   "metadata": {},
   "source": [
    "# Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d93d087-e26d-4036-be92-8fe797856c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'm2m100_418M'\n",
    "experiment = 'en-ha_finetune_base_model-1'\n",
    "dataset_name = 'data/en-ha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcb15db-3e10-4c2d-a788-9f3d755fa5f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"facebook/{model_name}\")\n",
    "# model = torch.nn.DataParallel(model, device_ids=[2, 3, 4])\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"facebook/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da91567",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f8621a-91a7-430f-abb8-b24099ba7ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from Hugging Face Hub\n",
    "original_train_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_train\", split='train')\n",
    "original_valid_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_valid\", split='train')  # Assuming the split is also 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the datasets to only include samples with a score > 0.6\n",
    "filtered_train_dataset = original_train_dataset.filter(lambda example: example['score'] > 0.6)\n",
    "filtered_valid_dataset = original_valid_dataset.filter(lambda example: example['score'] > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_dataset = filtered_train_dataset.remove_columns(['score', 'ref'])\n",
    "filtered_valid_dataset = filtered_valid_dataset.remove_columns(['score', 'ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_train.csv', split='train')\n",
    "valid_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_dev.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ha'],\n",
       "    num_rows: 9818\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.rename_column('ha', 'src')\n",
    "valid_dataset = valid_dataset.rename_column('ha', 'src')\n",
    "train_dataset = train_dataset.rename_column('en', 'mt')\n",
    "valid_dataset = valid_dataset.rename_column('en', 'mt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mt', 'src'],\n",
       "    num_rows: 9818\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = concatenate_datasets([train_dataset, filtered_train_dataset])\n",
    "valid_dataset = concatenate_datasets([valid_dataset, filtered_valid_dataset])  \n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "valid_dataset = valid_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/12638 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 12638/12638 [00:03<00:00, 3297.62 examples/s]\n",
      "Map: 100%|██████████| 1449/1449 [00:00<00:00, 3257.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the tokenized inputs and labels\n",
    "    return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "tokenized_valid_dataset = valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "# Create the DatasetDict\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,  # Directly assign the processed dataset\n",
    "    'validation': tokenized_valid_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12638\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47843c49-de6f-4f0e-b972-33aea21d6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({'train':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_train.csv')).shuffle(seed=seed),\n",
    "                        'validation':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_dev.csv')).shuffle(seed=seed),\n",
    "                        'test':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/test.csv')).shuffle(seed=seed),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbf50e-4404-4b78-ade7-3093930a1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'] = dataset['test'].rename_column('sentence_eng_Latn','en')\n",
    "dataset['test'] = dataset['test'].rename_column('sentence_hau_Latn','ha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab027a2-400a-4961-90b4-da9b28449308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09ae18-7223-405e-a273-4b6b39725f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [example for example in examples[src_lang]]\n",
    "    targets = [example for example in examples[tgt_lang]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0dc3d-64be-41f0-8527-17fcbb3cc657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee83e5f-9065-43f5-bc23-624fa8635ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0a643",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8ad4b70-d1f7-4407-b135-9ee631f83f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "wer = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7816c388-e691-4c08-874f-14fc97dac435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "    # removeme\n",
    "    #import warnings\n",
    "    #warnings.warn(f\"unprocessed preds: {preds[0]}\\n)\")\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) \n",
    "\n",
    "    # removeme\n",
    "    #warnings.warn(f\"unprocessed decoded labels: {tokenizer.batch_decode(labels)[0]}\\n)\")\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # remove me\n",
    "    #inputs = eval_preds.input_ids\n",
    "    #decoded_inputs = tokenizer.batch_decode(inputs)\n",
    "    \n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    bleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    metrics = {\"bleu\": bleu_result[\"score\"]}\n",
    "\n",
    "    flattened_decoded_labels = [' '.join([str(x) for x in l]) for l in decoded_labels]\n",
    "    wer_score = wer.compute(predictions=decoded_preds, references=flattened_decoded_labels)\n",
    "    metrics[\"wer\"] = wer_score\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    metrics[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c1dc421-b458-4edd-94de-c6b18fdffe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a61a2b25-0b90-4074-81c2-f80b6f0e26c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./lang_adapters/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=15,\n",
    "    warmup_steps=0,\n",
    "    # lr_scheduler_type='cosine_with_restarts',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a361ae7-3f1f-413a-bda9-1e6fe6ee4899",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5925' max='5925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5925/5925 1:27:33, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.263111</td>\n",
       "      <td>25.268500</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.241613</td>\n",
       "      <td>28.138300</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.234886</td>\n",
       "      <td>29.427100</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.232156</td>\n",
       "      <td>30.590900</td>\n",
       "      <td>0.995500</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.233843</td>\n",
       "      <td>31.648400</td>\n",
       "      <td>0.995200</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.236766</td>\n",
       "      <td>32.067500</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.239103</td>\n",
       "      <td>32.442500</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.244547</td>\n",
       "      <td>32.968200</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.248213</td>\n",
       "      <td>32.886000</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.250425</td>\n",
       "      <td>32.859900</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.254507</td>\n",
       "      <td>33.033900</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.258608</td>\n",
       "      <td>33.357500</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.261680</td>\n",
       "      <td>33.525800</td>\n",
       "      <td>0.995200</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.262691</td>\n",
       "      <td>33.600000</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.264149</td>\n",
       "      <td>33.583500</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>33.409200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2, 14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Yahyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is God Yahyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is John Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is John Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is John Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: God is: Johnyah 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1Yweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: God is: Yahweh 1:2,14; Romans 9;5; Colossians 2:9; Hebrews 1:8; 1 Yahweh 5:20.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5925, training_loss=0.17147542449492442, metrics={'train_runtime': 5253.6947, 'train_samples_per_second': 36.083, 'train_steps_per_second': 1.128, 'total_flos': 1.0270450485559296e+17, 'train_loss': 0.17147542449492442, 'epoch': 15.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = f'./0.6_base_model/{experiment}'\n",
    "\n",
    "\n",
    "if not os.path.exists(base_model_path):\n",
    "    os.makedirs(base_model_path)\n",
    "\n",
    "trainer.save_model(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1012/1012 [00:00<00:00, 2982.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/127 00:54 < 00:14, 1.84 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22620/3289696467.py:37: UserWarning: preds: “We have have fourmonth liveslong childrenirt un is not-smabetical and are to have withabetic,\" I heard.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_22620/3289696467.py:38: UserWarning: labels: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n"
     ]
    }
   ],
   "source": [
    "#use the saved model for evaluation\n",
    "#base_model_path = f'./0.6_base_model/{experiment}'\n",
    "#model = M2M100ForConditionalGeneration.from_pretrained(base_model_path)\n",
    "#model = model.to(device)\n",
    "test_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/test.csv', split='train')\n",
    "\n",
    "#rename 'sentence_eng_Latn' to 'en' and 'sentence_hau_Latn' to 'ha'\n",
    "test_dataset = test_dataset.rename_column('sentence_eng_Latn','mt')\n",
    "test_dataset = test_dataset.rename_column('sentence_hau_Latn','src')\n",
    "\n",
    "#tokenize the dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "#convert the dataset into dataset dict\n",
    "tokenized_test_dataset = DatasetDict({\n",
    "    'test': tokenized_test_dataset  # Directly assign the processed dataset\n",
    "})\n",
    "\n",
    "eval_results = trainer.evaluate(tokenized_test_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4250865578651428,\n",
       " 'eval_bleu': 15.0956,\n",
       " 'eval_wer': 0.9945,\n",
       " 'eval_gen_len': 34.2757,\n",
       " 'eval_runtime': 113.7833,\n",
       " 'eval_samples_per_second': 8.894,\n",
       " 'eval_steps_per_second': 1.116,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f27388ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/base_model/en-ha_finetune_base_model-1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-02.be.ucsc.edu/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Save model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-02.be.ucsc.edu/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./base_model/\u001b[39m\u001b[39m{\u001b[39;00mexperiment\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-02.be.ucsc.edu/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     os\u001b[39m.\u001b[39;49mmkdir(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/base_model/\u001b[39;49m\u001b[39m{\u001b[39;49;00mexperiment\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-02.be.ucsc.edu/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./base_model/\u001b[39m\u001b[39m{\u001b[39;00mexperiment\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/base_model/en-ha_finetune_base_model-1'"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "if not os.path.exists(f'./base_model/{experiment}'):\n",
    "    os.mkdir(f'/base_model/{experiment}')\n",
    "    \n",
    "trainer.save_model(f\"./base_model/{experiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval finetuned model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a928793b-3fa2-45ab-aec7-3b0634a32869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45c3692c-9109-4d77-92ef-80ae133e1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"./base_model/{experiment}\")\n",
    "# model = torch.nn.DataParallel(model, device_ids=[2, 3, 4])\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"./base_model/{experiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/test.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence_eng_Latn', 'sentence_hau_Latn'],\n",
       "    num_rows: 1012\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename 'sentence_eng_Latn' to 'en' and 'sentence_hau_Latn' to 'ha'\n",
    "test_dataset = test_dataset.rename_column('sentence_eng_Latn','mt')\n",
    "test_dataset = test_dataset.rename_column('sentence_hau_Latn','src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1012/1012 [00:00<00:00, 3020.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenize the dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dataset into dataset dict\n",
    "tokenized_test_dataset = DatasetDict({\n",
    "    'test': tokenized_test_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1012\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/127 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb Cell 46\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-02.be.ucsc.edu/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#run evaluation\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-02.be.ucsc.edu/home/spandey7/Reinforcement-Learning/m2m_100_finetune_base_model.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate(tokenized_test_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer.py:3011\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3008\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3010\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3011\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3012\u001b[0m     eval_dataloader,\n\u001b[1;32m   3013\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3014\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3015\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3016\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   3017\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   3018\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   3019\u001b[0m )\n\u001b[1;32m   3021\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3022\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer.py:3245\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3243\u001b[0m \u001b[39mif\u001b[39;00m preds_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3244\u001b[0m     logits \u001b[39m=\u001b[39m nested_numpify(preds_host)\n\u001b[0;32m-> 3245\u001b[0m     all_preds \u001b[39m=\u001b[39m logits \u001b[39mif\u001b[39;00m all_preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(all_preds, logits, padding_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m   3246\u001b[0m \u001b[39mif\u001b[39;00m inputs_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3247\u001b[0m     inputs_decode \u001b[39m=\u001b[39m nested_numpify(inputs_host)\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer_pt_utils.py:121\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    118\u001b[0m     new_tensors\n\u001b[1;32m    119\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;49;00m t, n \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(tensors, new_tensors))\n\u001b[1;32m    122\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer_pt_utils.py:121\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    118\u001b[0m     new_tensors\n\u001b[1;32m    119\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    122\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer_pt_utils.py:129\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[1;32m    126\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39;49mpadding_index)\n\u001b[1;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported type for concatenation: got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/trainer_pt_utils.py:100\u001b[0m, in \u001b[0;36mnumpy_pad_and_concatenate\u001b[0;34m(array1, array2, padding_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m array2 \u001b[39m=\u001b[39m atleast_1d(array2)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(array1\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m array1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m array2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mconcatenate((array1, array2), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    102\u001b[0m \u001b[39m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m    103\u001b[0m new_shape \u001b[39m=\u001b[39m (array1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m array2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mmax\u001b[39m(array1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], array2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m array1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run evaluation\n",
    "eval_results = trainer.evaluate(tokenized_test_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024e8a5-7972-4c42-a4e1-dbe23c20b7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.46018558740615845,\n",
       " 'eval_bleu': 14.5322,\n",
       " 'eval_wer': 0.9948,\n",
       " 'eval_gen_len': 34.2757,\n",
       " 'eval_runtime': 174.9085,\n",
       " 'eval_samples_per_second': 5.786,\n",
       " 'eval_steps_per_second': 1.446,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.7 Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from Hugging Face Hub\n",
    "original_train_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_train\", split='train')\n",
    "original_valid_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_valid\", split='train')\n",
    "\n",
    "new_filtered_train_dataset = original_train_dataset.filter(lambda example: example['score'] > 0.7)\n",
    "new_filtered_valid_dataset = original_valid_dataset.filter(lambda example: example['score'] > 0.7)\n",
    "new_filtered_train_dataset = new_filtered_train_dataset.remove_columns(['score', 'ref'])\n",
    "new_filtered_valid_dataset = new_filtered_valid_dataset.remove_columns(['score', 'ref'])\n",
    "\n",
    "train_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_train.csv', split='train')\n",
    "valid_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_dev.csv', split='train')\n",
    "\n",
    "train_dataset = train_dataset.rename_column('ha', 'src')\n",
    "valid_dataset = valid_dataset.rename_column('ha', 'src')\n",
    "train_dataset = train_dataset.rename_column('en', 'mt')\n",
    "valid_dataset = valid_dataset.rename_column('en', 'mt')\n",
    "\n",
    "new_train_dataset = concatenate_datasets([train_dataset, new_filtered_train_dataset])\n",
    "new_valid_dataset = concatenate_datasets([valid_dataset, new_filtered_valid_dataset])  \n",
    "new_train_dataset = new_train_dataset.shuffle(seed=42)\n",
    "new_valid_dataset = new_valid_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mt', 'src'],\n",
       "    num_rows: 9818\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src', 'mt'],\n",
       "    num_rows: 802\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_filtered_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mt', 'src'],\n",
       "    num_rows: 10620\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10620 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 10620/10620 [00:03<00:00, 3262.77 examples/s]\n",
      "Map: 100%|██████████| 1199/1199 [00:00<00:00, 3336.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the tokenized inputs and labels\n",
    "    return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "new_tokenized_train_dataset = new_train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "new_tokenized_valid_dataset = new_valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "# Create the DatasetDict\n",
    "new_tokenized_dataset = DatasetDict({\n",
    "    'train': new_tokenized_train_dataset,  # Directly assign the processed dataset\n",
    "    'validation': new_tokenized_valid_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10620\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1199\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "    # removeme\n",
    "    #import warnings\n",
    "    #warnings.warn(f\"unprocessed preds: {preds[0]}\\n)\")\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) \n",
    "\n",
    "    # removeme\n",
    "    #warnings.warn(f\"unprocessed decoded labels: {tokenizer.batch_decode(labels)[0]}\\n)\")\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # remove me\n",
    "    #inputs = eval_preds.input_ids\n",
    "    #decoded_inputs = tokenizer.batch_decode(inputs)\n",
    "    \n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    bleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    metrics = {\"bleu\": bleu_result[\"score\"]}\n",
    "\n",
    "    flattened_decoded_labels = [' '.join([str(x) for x in l]) for l in decoded_labels]\n",
    "    wer_score = wer.compute(predictions=decoded_preds, references=flattened_decoded_labels)\n",
    "    metrics[\"wer\"] = wer_score\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    metrics[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./lang_adapters/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=15,\n",
    "    warmup_steps=0,\n",
    "    # lr_scheduler_type='cosine_with_restarts',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=new_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=new_tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4980' max='4980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4980/4980 1:07:35, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.238500</td>\n",
       "      <td>0.258932</td>\n",
       "      <td>27.087200</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.247632</td>\n",
       "      <td>28.919900</td>\n",
       "      <td>0.994400</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182100</td>\n",
       "      <td>0.243424</td>\n",
       "      <td>29.627800</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.244643</td>\n",
       "      <td>30.753300</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.246674</td>\n",
       "      <td>31.211700</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.250211</td>\n",
       "      <td>31.893600</td>\n",
       "      <td>0.994400</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>0.254414</td>\n",
       "      <td>31.998400</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.259981</td>\n",
       "      <td>32.144800</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.267587</td>\n",
       "      <td>32.227300</td>\n",
       "      <td>0.995300</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.269285</td>\n",
       "      <td>32.774900</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.273092</td>\n",
       "      <td>32.843900</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.275070</td>\n",
       "      <td>32.897700</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.278494</td>\n",
       "      <td>32.953000</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.279527</td>\n",
       "      <td>32.951600</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.281194</td>\n",
       "      <td>32.964500</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>34.075100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that fact that I province is the theited by Sh Shia and Sunni.s.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few that You province is where theited by Sh Shia and Sunni peoples.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few that I province is where theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few that I province is the theited by Sh Shia and Sunni communs.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: Here would like to tell this opportunity to tell that few that I province is the theited by Sh Shia and Sunni communs.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni communs.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: Here would like to make this opportunity to tell that few that You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: Here would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni.s.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: Here would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: I would like to tell this opportunity to tell that few in You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: Here would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: Here would like to tell this opportunity to tell that few that You province is the theited by Sh Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: I would like to take this opportunity to mention a point. Your province is inhabited by both Shia and Sunni Muslims.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4980, training_loss=0.09708952237085167, metrics={'train_runtime': 4056.3316, 'train_samples_per_second': 39.272, 'train_steps_per_second': 1.228, 'total_flos': 8.63049407791104e+16, 'train_loss': 0.09708952237085167, 'epoch': 15.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = f'./0.7_base_model/{experiment}'\n",
    "\n",
    "if not os.path.exists(base_model_path):\n",
    "    os.makedirs(base_model_path)\n",
    "\n",
    "trainer.save_model(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1012/1012 [00:00<00:00, 3057.94 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17191/104191538.py:40: UserWarning: preds: OnceWe are are fourmonth hardlong couock- has not-adabetical and have to have thabetic,\" I is.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17191/104191538.py:41: UserWarning: labels: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/test.csv', split='train')\n",
    "\n",
    "#rename 'sentence_eng_Latn' to 'en' and 'sentence_hau_Latn' to 'ha'\n",
    "test_dataset = test_dataset.rename_column('sentence_eng_Latn','mt')\n",
    "test_dataset = test_dataset.rename_column('sentence_hau_Latn','src')\n",
    "\n",
    "#tokenize the dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "#convert the dataset into dataset dict\n",
    "tokenized_test_dataset = DatasetDict({\n",
    "    'test': tokenized_test_dataset  # Directly assign the processed dataset\n",
    "})\n",
    "\n",
    "new_eval_results = trainer.evaluate(tokenized_test_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.42892614006996155,\n",
       " 'eval_bleu': 15.2755,\n",
       " 'eval_wer': 0.9946,\n",
       " 'eval_gen_len': 34.2757,\n",
       " 'eval_runtime': 110.6888,\n",
       " 'eval_samples_per_second': 9.143,\n",
       " 'eval_steps_per_second': 1.147,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_train.csv', split='train')\n",
    "valid_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_dev.csv', split='train')\n",
    "\n",
    "train_dataset = train_dataset.rename_column('ha', 'src')\n",
    "valid_dataset = valid_dataset.rename_column('ha', 'src')\n",
    "train_dataset = train_dataset.rename_column('en', 'mt')\n",
    "valid_dataset = valid_dataset.rename_column('en', 'mt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mt', 'src'],\n",
       "    num_rows: 9818\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/9818 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 9818/9818 [00:02<00:00, 3339.66 examples/s]\n",
      "Map: 100%|██████████| 1113/1113 [00:00<00:00, 3206.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the tokenized inputs and labels\n",
    "    return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "og_tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "og_tokenized_valid_dataset = valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "# Create the DatasetDict\n",
    "og_tokenized_dataset = DatasetDict({\n",
    "    'train': og_tokenized_train_dataset,  # Directly assign the processed dataset\n",
    "    'validation': og_tokenized_valid_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9818\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "    # removeme\n",
    "    #import warnings\n",
    "    #warnings.warn(f\"unprocessed preds: {preds[0]}\\n)\")\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) \n",
    "\n",
    "    # removeme\n",
    "    #warnings.warn(f\"unprocessed decoded labels: {tokenizer.batch_decode(labels)[0]}\\n)\")\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # remove me\n",
    "    #inputs = eval_preds.input_ids\n",
    "    #decoded_inputs = tokenizer.batch_decode(inputs)\n",
    "    \n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    bleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    metrics = {\"bleu\": bleu_result[\"score\"]}\n",
    "\n",
    "    flattened_decoded_labels = [' '.join([str(x) for x in l]) for l in decoded_labels]\n",
    "    wer_score = wer.compute(predictions=decoded_preds, references=flattened_decoded_labels)\n",
    "    metrics[\"wer\"] = wer_score\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    metrics[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./lang_adapters/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=15,\n",
    "    warmup_steps=0,\n",
    "    # lr_scheduler_type='cosine_with_restarts',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=og_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=og_tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4605' max='4605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4605/4605 59:49, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.297524</td>\n",
       "      <td>23.138400</td>\n",
       "      <td>0.996300</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.268421</td>\n",
       "      <td>26.638400</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.257582</td>\n",
       "      <td>28.859200</td>\n",
       "      <td>0.995400</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.197300</td>\n",
       "      <td>0.254482</td>\n",
       "      <td>29.713700</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>0.255224</td>\n",
       "      <td>30.268500</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.256460</td>\n",
       "      <td>30.893600</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>0.261120</td>\n",
       "      <td>31.531100</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.265503</td>\n",
       "      <td>31.701200</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.270239</td>\n",
       "      <td>31.923900</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.274925</td>\n",
       "      <td>32.194800</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.277787</td>\n",
       "      <td>32.474400</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.280964</td>\n",
       "      <td>32.604500</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.283483</td>\n",
       "      <td>32.731300</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.285078</td>\n",
       "      <td>32.785300</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.285858</td>\n",
       "      <td>32.842500</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>35.245300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today in our neighic provinaries there there are students schij scholars and are proud of this the of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our hallic seminaries are there are high schiji scholars and are proud of this the of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries, there are also schiji scholars and are proud of this the of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries, there are high schiji scholars and are proud of this the of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today in our schoolsic seminaries there there are high schiji scholars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries, there are high schiji scholars and are proud of this a of theij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries, there are high schiji scholars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are high schiji scholars and are comm of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are students schiji scholars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are students schiji studentsolars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are certain schiji scholars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are certain schiji studentsolars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are students schiji studentsolars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are certain schiji studentsolars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: Today there our schoolsic seminaries there there are students schiji studentsolars and are proud of this a of Basij.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: Today in our Islamic seminaries, there are great basiji scholars who are proud of being members of Basij.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4605, training_loss=0.20949620604838662, metrics={'train_runtime': 3589.6463, 'train_samples_per_second': 41.026, 'train_steps_per_second': 1.283, 'total_flos': 7.978737368825856e+16, 'train_loss': 0.20949620604838662, 'epoch': 15.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = f'./og_base_model/{experiment}'\n",
    "\n",
    "if not os.path.exists(trained_model_path):\n",
    "    os.makedirs(trained_model_path)\n",
    "\n",
    "trainer.save_model(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1012/1012 [00:00<00:00, 3043.94 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 01:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17580/104191538.py:40: UserWarning: preds: “We are have 4month longlong peopleock- is without-sabetical and have to have thabetic,\" I says.\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_17580/104191538.py:41: UserWarning: labels: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/test.csv', split='train')\n",
    "\n",
    "#rename 'sentence_eng_Latn' to 'en' and 'sentence_hau_Latn' to 'ha'\n",
    "test_dataset = test_dataset.rename_column('sentence_eng_Latn','mt')\n",
    "test_dataset = test_dataset.rename_column('sentence_hau_Latn','src')\n",
    "\n",
    "#tokenize the dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "#convert the dataset into dataset dict\n",
    "tokenized_test_dataset = DatasetDict({\n",
    "    'test': tokenized_test_dataset  # Directly assign the processed dataset\n",
    "})\n",
    "\n",
    "new_eval_results = trainer.evaluate(tokenized_test_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41901615262031555,\n",
       " 'eval_bleu': 15.2866,\n",
       " 'eval_wer': 0.9947,\n",
       " 'eval_gen_len': 34.2757,\n",
       " 'eval_runtime': 118.8275,\n",
       " 'eval_samples_per_second': 8.517,\n",
       " 'eval_steps_per_second': 1.069,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.8 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
