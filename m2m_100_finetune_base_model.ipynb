{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1572d-e876-433f-a076-07e0e138f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"adapter-transformers@git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3551c529-fcb2-458d-8e5c-4544876eba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric, load_from_disk, concatenate_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, TrainingArguments, Trainer\n",
    "#from transformers.adapters import AdapterTrainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef787507-63c0-45ca-ba66-d56f4141dc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1631e252-26e7-4ec8-ac5b-efdd94a678fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7895b82a-ddd2-481b-8932-0907b73462a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ea527",
   "metadata": {},
   "source": [
    "# Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d93d087-e26d-4036-be92-8fe797856c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'm2m100_418M'\n",
    "experiment = 'en-ha_finetune_base_model-1'\n",
    "dataset_name = 'data/en-ha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcb15db-3e10-4c2d-a788-9f3d755fa5f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"facebook/{model_name}\")\n",
    "# model = torch.nn.DataParallel(model, device_ids=[2, 3, 4])\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"facebook/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da91567",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f8621a-91a7-430f-abb8-b24099ba7ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from Hugging Face Hub\n",
    "original_train_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_train\", split='train')\n",
    "original_valid_dataset = load_dataset(\"pranjali97/ha-en_RL-grow1_valid\", split='train')  # Assuming the split is also 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the datasets to only include samples with a score > 0.6\n",
    "filtered_train_dataset = original_train_dataset.filter(lambda example: example['score'] > 0.6)\n",
    "filtered_valid_dataset = original_valid_dataset.filter(lambda example: example['score'] > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_dataset = filtered_train_dataset.remove_columns(['score', 'ref'])\n",
    "filtered_valid_dataset = filtered_valid_dataset.remove_columns(['score', 'ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 882.27it/s]\n",
      "Generating train split: 1113 examples [00:00, 86095.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_train.csv', split='train')\n",
    "valid_dataset = load_dataset('csv', data_files='/home/spandey7/Language-Adapters/Data/en-ha/cleaned_dev.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.rename_column('ha', 'mt')\n",
    "valid_dataset = valid_dataset.rename_column('ha', 'mt')\n",
    "train_dataset = train_dataset.rename_column('en', 'src')\n",
    "valid_dataset = valid_dataset.rename_column('en', 'src')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = concatenate_datasets([train_dataset, filtered_train_dataset])\n",
    "valid_dataset = concatenate_datasets([valid_dataset, filtered_valid_dataset])  \n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "valid_dataset = valid_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/15458 [00:00<?, ? examples/s]/home/spandey7/miniconda3/envs/capstone/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 15458/15458 [00:04<00:00, 3212.40 examples/s]\n",
      "Map: 100%|██████████| 1785/1785 [00:00<00:00, 3491.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src']  # Hausa sentences\n",
    "    targets = examples['mt']  # English translations\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    # Return the tokenized inputs and labels\n",
    "    return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "tokenized_valid_dataset = valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=['src',  'mt']  # Specify the correct columns to remove\n",
    ")\n",
    "\n",
    "# Create the DatasetDict\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,  # Directly assign the processed dataset\n",
    "    'validation': tokenized_valid_dataset  # Directly assign the processed dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15458\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1785\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47843c49-de6f-4f0e-b972-33aea21d6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({'train':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_train.csv')).shuffle(seed=seed),\n",
    "                        'validation':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_dev.csv')).shuffle(seed=seed),\n",
    "                        'test':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/test.csv')).shuffle(seed=seed),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbf50e-4404-4b78-ade7-3093930a1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'] = dataset['test'].rename_column('sentence_eng_Latn','en')\n",
    "dataset['test'] = dataset['test'].rename_column('sentence_hau_Latn','ha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab027a2-400a-4961-90b4-da9b28449308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09ae18-7223-405e-a273-4b6b39725f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [example for example in examples[src_lang]]\n",
    "    targets = [example for example in examples[tgt_lang]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0dc3d-64be-41f0-8527-17fcbb3cc657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee83e5f-9065-43f5-bc23-624fa8635ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0a643",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8ad4b70-d1f7-4407-b135-9ee631f83f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "wer = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7816c388-e691-4c08-874f-14fc97dac435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "    # removeme\n",
    "    #import warnings\n",
    "    #warnings.warn(f\"unprocessed preds: {preds[0]}\\n)\")\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) \n",
    "\n",
    "    # removeme\n",
    "    #warnings.warn(f\"unprocessed decoded labels: {tokenizer.batch_decode(labels)[0]}\\n)\")\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # remove me\n",
    "    #inputs = eval_preds.input_ids\n",
    "    #decoded_inputs = tokenizer.batch_decode(inputs)\n",
    "    \n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    bleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    metrics = {\"bleu\": bleu_result[\"score\"]}\n",
    "\n",
    "    flattened_decoded_labels = [' '.join([str(x) for x in l]) for l in decoded_labels]\n",
    "    wer_score = wer.compute(predictions=decoded_preds, references=flattened_decoded_labels)\n",
    "    metrics[\"wer\"] = wer_score\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    metrics[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    metrics = {k: round(v, 4) for k, v in metrics.items()}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c1dc421-b458-4edd-94de-c6b18fdffe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a61a2b25-0b90-4074-81c2-f80b6f0e26c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./lang_adapters/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=15,\n",
    "    warmup_steps=0,\n",
    "    # lr_scheduler_type='cosine_with_restarts',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a361ae7-3f1f-413a-bda9-1e6fe6ee4899",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='442' max='14505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  442/14505 01:32 < 49:06, 4.77 it/s, Epoch 0.46/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27388ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "if not os.path.exists(f'./base_model/{experiment}'):\n",
    "    os.mkdir(f'./base_model/{experiment}')\n",
    "    \n",
    "trainer.save_model(f\"./base_model/{experiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval finetuned model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928793b-3fa2-45ab-aec7-3b0634a32869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3692c-9109-4d77-92ef-80ae133e1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"./base_model/{experiment}\")\n",
    "# model = torch.nn.DataParallel(model, device_ids=[2, 3, 4])\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"./base_model/{experiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4e6d2-f884-4a9c-a4ff-c1fa70b55da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_outputs = trainer.evaluate(tokenized_dataset['test']) #, forced_bos_token_id=tokenizer.get_lang_id(\"ha\")\n",
    "#test_output_texts = tokenizer.batch_decode(torch.LongTensor(test_outputs.predictions), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024e8a5-7972-4c42-a4e1-dbe23c20b7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
